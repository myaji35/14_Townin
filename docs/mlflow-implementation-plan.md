# MLflow ë„ì… ê³„íšì„œ

**ì‘ì„±ì¼**: 2024-12-21
**ë²„ì „**: 1.0
**ìƒíƒœ**: Phase 1ë¶€í„° í•„ìˆ˜ ë„ì…
**ê´€ë ¨ PRD ì„¹ì…˜**: 5. Insurance GraphRAG, 9.1 RAG ì„±ëŠ¥ ì§€í‘œ, 10.1 ê¸°ìˆ ì  ë¦¬ìŠ¤í¬

---

## 1. ê°œìš”

### 1.1 ëª©ì 

MLflowë¥¼ Townin í”„ë¡œì íŠ¸ì˜ **í•µì‹¬ ì¸í”„ë¼**ë¡œ ë„ì…í•˜ì—¬ AI/ML ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

**íƒ€ë‹¹ì„± ì ìˆ˜**: 95/100 (ê°•ë ¥ ê¶Œì¥)

### 1.2 MLflowê°€ í•„ìˆ˜ì¸ ì´ìœ 

```
Towninì€ LLM/GraphRAG ì¤‘ì‹¬ ì•„í‚¤í…ì²˜:
â”œâ”€â”€ Insurance GraphRAG ì—”ì§„ (í•µì‹¬)
â”œâ”€â”€ ë©€í‹°ëª¨ë‹¬ AI (ì „ë‹¨ì§€ ìŠ¤ìºë„ˆ, ì‚¬ê³  í˜„ì¥ ë¶„ì„)
â”œâ”€â”€ FP ì½”íŒŒì¼ëŸ¿ (ì•½ê´€ ê²€ìƒ‰, ê´‘ê³  ì‹¬ì˜)
â””â”€â”€ ì‚¬ê¸° íƒì§€ ëª¨ë¸

â†’ MLflow ì—†ì´ëŠ” ì„±ê³µ ì§€í‘œ ì¸¡ì • ë° ë¹„ìš© ê´€ë¦¬ ë¶ˆê°€ëŠ¥
```

### 1.3 pm4py vs MLflow ë¹„êµ

| ì¸¡ë©´ | pm4py | MLflow |
|------|-------|--------|
| **í•„ìˆ˜ì„±** | ì„ íƒì  (ì°¨ë³„í™”) | **í•„ìˆ˜** (í•µì‹¬ ì¸í”„ë¼) |
| **ë„ì… ì‹œê¸°** | Phase 2ë¶€í„° | **Phase 1ë¶€í„°** |
| **íƒ€ë‹¹ì„±** | 85/100 | **95/100** |
| **ìš©ë„** | Process Mining | LLM/GraphRAG ì‹¤í—˜ ì¶”ì  |

---

## 2. MLflow í•µì‹¬ ê¸°ëŠ¥

### 2.1 LLM Tracking & Tracing

**2025ë…„ ì£¼ìš” ê°œì„ ì‚¬í•­**:
- GenAI/LLM ì›Œí¬í”Œë¡œìš° ì „ìš© ê¸°ëŠ¥ ê°•í™”
- OpenTelemetry í˜¸í™˜ íŠ¸ë ˆì´ì‹±
- LangChain ìë™ ë¡œê¹…: `mlflow.langchain.autolog()`

**ê¸°ëŠ¥**:
```python
import mlflow

mlflow.langchain.autolog()

with mlflow.start_run(run_name="graphrag_v1.2"):
    # ìë™ìœ¼ë¡œ ì¶”ì ë¨:
    # - Prompt ì…ë ¥/ì¶œë ¥
    # - LLM íŒŒë¼ë¯¸í„° (temperature, model)
    # - Token ì‚¬ìš©ëŸ‰ (ë¹„ìš© ê³„ì‚° ê°€ëŠ¥)
    # - ì‹¤í–‰ ì‹œê°„
    # - Trace (ë‹¨ê³„ë³„ ì‹¤í–‰ íë¦„)

    result = graphrag_pipeline.run(query)
```

### 2.2 Model Registry

**ë²„ì „ ê´€ë¦¬**:
- ëª¨ë¸ ìë™ ë²„ì „ ì¶”ì 
- Staging â†’ Production ìŠ¹ê²© ì›Œí¬í”Œë¡œìš°
- ë¡¤ë°± ê¸°ëŠ¥
- ì¬í˜„ì„± ë³´ì¥ (ì–´ë–¤ ë°ì´í„°/íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí–ˆëŠ”ì§€ ì¶”ì )

### 2.3 Prompt Registry

**í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬**:
```python
# í”„ë¡¬í”„íŠ¸ë¥¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
mlflow.log_param("prompt_version", "insurance_qa_v2.3")

# A/B í…ŒìŠ¤íŠ¸
with mlflow.start_run(run_name="prompt_a"):
    result_a = run_with_prompt("insurance_qa_v2.3")

with mlflow.start_run(run_name="prompt_b"):
    result_b = run_with_prompt("insurance_qa_v2.4")

# ì„±ëŠ¥ ë¹„êµ
mlflow.log_metric("faithfulness_a", 0.93)
mlflow.log_metric("faithfulness_b", 0.95)  # v2.4ê°€ ë” ì¢‹ìŒ!
```

### 2.4 Evaluation

**LLM ì¶œë ¥ í’ˆì§ˆ í‰ê°€**:
```python
from mlflow.metrics import make_judge

# ì»¤ìŠ¤í…€ í‰ê°€ Judge ìƒì„±
insurance_judge = make_judge(
    domain="insurance",
    criteria="ë‹µë³€ì´ ì•½ê´€ì— ê¸°ë°˜í•˜ëŠ”ì§€, í™˜ê°ì´ ì—†ëŠ”ì§€"
)

# PRD 9.1 RAG ì„±ëŠ¥ ì§€í‘œ ìë™ ì¸¡ì •
metrics = mlflow.evaluate(
    model=graphrag_model,
    data=test_queries,
    metrics=[
        "retrieval_precision",  # > 85% ëª©í‘œ
        "faithfulness",          # > 95% ëª©í‘œ
        "answer_relevancy"       # > 90% ëª©í‘œ
    ]
)
```

### 2.5 Token Usage Tracking

**ë¹„ìš© ê´€ë¦¬** (PRD 10.1 ë¦¬ìŠ¤í¬ ì™„í™”):
```python
mlflow.langchain.autolog(log_token_usage=True)

# ìë™ìœ¼ë¡œ ì¶”ì ë¨
daily_cost = (
    input_tokens * 0.005 / 1000 +   # GPT-4o input
    output_tokens * 0.015 / 1000     # GPT-4o output
)

mlflow.log_metric("daily_llm_cost", daily_cost)

# ëª©í‘œ: ì²­êµ¬ë‹¹ ì²˜ë¦¬ ë¹„ìš© < $20
```

---

## 3. Townin ì ìš© ì‹œë‚˜ë¦¬ì˜¤

### 3.1 GraphRAG íŒŒì´í”„ë¼ì¸ ìµœì í™” â­â­â­â­â­

**PRD ì—°ê³„**:
- **FR-CORE-001**: ë°ì´í„° ìˆ˜ì§‘ ë° ì¸ë±ì‹±
- **FR-CORE-002**: ê²€ìƒ‰ ë° ì§ˆì˜ ì²˜ë¦¬
- **9.1 RAG ì„±ëŠ¥ ì§€í‘œ**: ê²€ìƒ‰ ì •ë°€ë„, ì¶©ì‹¤ë„, ë‹µë³€ ê´€ë ¨ì„±

**êµ¬í˜„**:
```python
import mlflow
from langchain.chains import GraphCypherQAChain

mlflow.langchain.autolog()

# FR-CORE-001: ì²­í‚¹ ì „ëµ ì‹¤í—˜
with mlflow.start_run(run_name="chunking_strategy_comparison"):
    for strategy in ["semantic", "fixed_size", "paragraph"]:
        mlflow.log_param("chunking_strategy", strategy)
        mlflow.log_param("chunk_size", 512)

        # Leiden í´ëŸ¬ìŠ¤í„°ë§
        mlflow.log_param("leiden_resolution", 1.0)

        # ê·¸ë˜í”„ êµ¬ì¶•
        graph = build_knowledge_graph(strategy=strategy)

        # FR-CORE-002: ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
        metrics = evaluate_retrieval(test_queries, graph)

        # PRD 9.1 ëª©í‘œì¹˜ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        mlflow.log_metrics({
            "retrieval_precision": metrics['precision'],  # ëª©í‘œ: > 85%
            "faithfulness": metrics['faithfulness'],       # ëª©í‘œ: > 95%
            "answer_relevancy": metrics['relevancy'],      # ëª©í‘œ: > 90%
            "graph_coverage": metrics['coverage']          # ëª©í‘œ: > 90%
        })

# ìµœê³  ì„±ëŠ¥ íŒŒë¼ë¯¸í„° ìë™ ì„ íƒ
best_run = mlflow.search_runs(
    filter_string="metrics.faithfulness > 0.95",
    order_by=["metrics.retrieval_precision DESC"]
).iloc[0]

print(f"Best chunking strategy: {best_run['params.chunking_strategy']}")
```

**íš¨ê³¼**:
- PRD ëª©í‘œì¹˜ ë‹¬ì„± ëª¨ë‹ˆí„°ë§ ìë™í™”
- íŒŒë¼ë¯¸í„° íŠœë‹ ì´ë ¥ ë³´ì¡´
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜

### 3.2 ì „ë‹¨ì§€ ìŠ¤ìºë„ˆ ìµœì í™” â­â­â­â­

**PRD ì—°ê³„**:
- **3.2 AI ì „ë‹¨ì§€ ìŠ¤ìºë„ˆ**: OCR/Vision AI
- **FR-CORE-003.1**: ì´ë¯¸ì§€ ë¶„ì„

**êµ¬í˜„**:
```python
# Vision AI ëª¨ë¸ ë¹„êµ ì‹¤í—˜
with mlflow.start_run(run_name="flyer_ocr_optimization"):
    for model in ["gpt-4-vision", "claude-3-5-sonnet-vision"]:
        mlflow.log_param("vision_model", model)
        mlflow.log_param("max_tokens", 1000)

        # 100ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        results = process_test_flyers(test_set, model=model)

        mlflow.log_metrics({
            "product_extraction_accuracy": results['product_acc'],
            "price_extraction_accuracy": results['price_acc'],
            "avg_cost_per_flyer": results['cost'],
            "processing_time_ms": results['latency']
        })

        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ì¥
        mlflow.log_artifact("sample_flyer_001.jpg")

# ëª©í‘œ: ì •í™•ë„ 90% + ë¹„ìš© < $0.05
```

### 3.3 FP ì½”íŒŒì¼ëŸ¿ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ â­â­â­â­â­

**PRD ì—°ê³„**:
- **4.1 FP ì „ìš© AI ì½”íŒŒì¼ëŸ¿**: ë¦¬ë“œ ë§¤ì¹­, ì•½ê´€ ê²€ìƒ‰
- **4.2 AI ê´‘ê³  ì‹¬ì˜ ìƒì„±ê¸°**: Compliance
- **FR-INS-003**: í™˜ê° ë°©ì§€

**í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬**:
```python
# í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸
prompts = {
    "v2.3": "ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•½ê´€ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
    "v2.4": "ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì•½ê´€ ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ 'í™•ì¸ í•„ìš”'ë¼ê³  ë‹µí•˜ì„¸ìš”."
}

for version, prompt in prompts.items():
    with mlflow.start_run(run_name=f"fp_copilot_{version}"):
        mlflow.log_param("prompt_version", version)
        mlflow.log_param("temperature", 0.3)
        mlflow.log_param("model", "gpt-4o")

        # í…ŒìŠ¤íŠ¸
        results = run_copilot_test(prompt, test_cases)

        # FR-INS-003.2: í™˜ê° ë°©ì§€ (ì‹ ë¢°ë„ ì ìˆ˜)
        mlflow.log_metrics({
            "hallucination_rate": results['hallucination'],  # ëª©í‘œ: < 5%
            "citation_accuracy": results['citation'],         # ëª©í‘œ: > 95%
            "confidence_score": results['confidence']
        })

# v2.4ê°€ í™˜ê°ë¥  3%ë¡œ ëª©í‘œ ë‹¬ì„±!
```

### 3.4 ì‚¬ê¸° íƒì§€ ëª¨ë¸ ìš´ì˜ â­â­â­â­

**PRD ì—°ê³„**:
- **FR-INS-001.3**: ì‚¬ê¸° íƒì§€ ë° êµì°¨ ê²€ì¦
- **9.2 ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ**: ì§í†µ ì²˜ë¦¬ìœ¨(STP) 60% ëª©í‘œ

**ëª¨ë¸ ë°°í¬**:
```python
# ì‚¬ê¸° íƒì§€ ëª¨ë¸ í•™ìŠµ
with mlflow.start_run(run_name="fraud_detector_v3"):
    mlflow.log_params({
        "algorithm": "random_forest",
        "n_estimators": 100,
        "max_depth": 10
    })

    # í•™ìŠµ
    model = train_fraud_detector(training_data)

    # í‰ê°€
    mlflow.log_metrics({
        "precision": 0.92,
        "recall": 0.88,
        "f1_score": 0.90
    })

    # ëª¨ë¸ ì €ì¥
    mlflow.sklearn.log_model(model, "fraud_detector")

# Model Registryì— ë“±ë¡
mlflow.register_model(
    "runs:/<run_id>/fraud_detector",
    "FraudDetector"
)

# Staging â†’ Production ìŠ¹ê²©
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="FraudDetector",
    version=3,
    stage="Production"
)

# STP 60% ëª©í‘œ ë‹¬ì„± ëª¨ë‹ˆí„°ë§
```

### 3.5 ë¹„ìš© ìµœì í™” (GraphRAG) â­â­â­â­â­

**PRD ì—°ê³„**:
- **10.1 ë¦¬ìŠ¤í¬ 1**: GraphRAG êµ¬ì¶• ë¹„ìš© ê³¼ë‹¤

**í† í° ë¹„ìš© ì¶”ì **:
```python
mlflow.langchain.autolog(log_token_usage=True)

# ì¦ë¶„ ì—…ë°ì´íŠ¸ vs ì „ì²´ ì¬ì²˜ë¦¬ ë¹„êµ
strategies = ["incremental", "full_rebuild"]

for strategy in strategies:
    with mlflow.start_run(run_name=f"graphrag_build_{strategy}"):
        mlflow.log_param("update_strategy", strategy)

        # ê·¸ë˜í”„ êµ¬ì¶•
        start_time = time.time()
        graph, token_usage = build_graph(strategy=strategy)
        duration = time.time() - start_time

        # ë¹„ìš© ê³„ì‚°
        cost = (
            token_usage['input'] * 0.005 / 1000 +
            token_usage['output'] * 0.015 / 1000
        )

        mlflow.log_metrics({
            "build_time_seconds": duration,
            "total_tokens": token_usage['total'],
            "cost_usd": cost
        })

# Insight: ì¦ë¶„ ì—…ë°ì´íŠ¸ê°€ 90% ë¹„ìš© ì ˆê°!
```

---

## 4. ë„ì… ì „ëµ

### 4.1 Phaseë³„ MLflow í™œìš©

```
Phase 1 (ê¸°íš ë‹¨ê³„, í˜„ì¬ - 2025 Q1):
âœ… MLflow ì¸í”„ë¼ êµ¬ì¶•
âœ… ì „ë‹¨ì§€ OCR ì‹¤í—˜ ì¶”ì  ì‹œì‘
âœ… í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ PoC

Phase 2 (ê°œë°œ ì°©ìˆ˜, 2025 Q2 - Q4):
âœ… GraphRAG íŒŒì´í”„ë¼ì¸ ìµœì í™” (í•µì‹¬!)
âœ… IoT ì´ìƒ íƒì§€ ëª¨ë¸ ì‹¤í—˜
âœ… í”„ë¡¬í”„íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë³¸ê²© ì‚¬ìš©

Phase 3 (ìˆ˜ìµí™”, 2026 Q1 - Q4):
âœ… ì‚¬ê¸° íƒì§€ ëª¨ë¸ ìš´ì˜
âœ… RAG ì„±ëŠ¥ ì§€í‘œ ëŒ€ì‹œë³´ë“œ
âœ… FP ì½”íŒŒì¼ëŸ¿ A/B í…ŒìŠ¤íŠ¸

Phase 4 (ê¸€ë¡œë²Œ í™•ì¥, 2027+):
âœ… êµ­ê°€ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
âœ… ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬
âœ… Databricks Managed MLflow ë§ˆì´ê·¸ë ˆì´ì…˜
```

### 4.2 ì¸í”„ë¼ êµ¬ì„±

#### Self-hosted MLflow (Phase 1-2 ê¶Œì¥)

```yaml
# docker-compose.yml
version: '3.8'

services:
  # MLflow Tracking Server
  mlflow:
    image: mlflow/mlflow:latest
    container_name: townin-mlflow
    ports:
      - "5000:5000"
    environment:
      - BACKEND_STORE_URI=postgresql://mlflow_user:password@postgres:5432/mlflow
      - ARTIFACT_ROOT=s3://townin-mlflow-artifacts
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    command: >
      mlflow server
      --backend-store-uri postgresql://mlflow_user:password@postgres:5432/mlflow
      --default-artifact-root s3://townin-mlflow-artifacts
      --host 0.0.0.0
      --port 5000
    depends_on:
      - postgres

  # PostgreSQL (ì‹¤í—˜ ë©”íƒ€ë°ì´í„°)
  postgres:
    image: postgres:15
    container_name: mlflow-postgres
    environment:
      - POSTGRES_USER=mlflow_user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=mlflow
    volumes:
      - mlflow_postgres_data:/var/lib/postgresql/data

  # Townin AI Engine (FastAPI + LangChain + pm4py)
  ai-engine:
    build: ./ai-engine
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - mlflow

volumes:
  mlflow_postgres_data:
```

#### ë¹„ìš© ì˜ˆìƒ

| êµ¬ì„± | ì›” ë¹„ìš© (USD) | í•œí™” (â‚©) |
|------|-------------|----------|
| **Self-hosted (Phase 1-2)** | | |
| - EC2 t3.medium | $30 | â‚©39,000 |
| - RDS PostgreSQL | $20 | â‚©26,000 |
| - S3 (ì•„í‹°íŒ©íŠ¸) | $10 | â‚©13,000 |
| **ì†Œê³„** | **$60** | **â‚©78,000** |
| | | |
| **Databricks Managed (Phase 3+)** | | |
| - MLflow ê´€ë¦¬í˜• | $500-2,000 | â‚©650K-2.6M |

**ê¶Œì¥**: Phase 1-2ëŠ” Self-hosted ($60/ì›”), Phase 3ë¶€í„° Managed ê³ ë ¤

### 4.3 ê¸°ìˆ  ìŠ¤íƒ í†µí•©

```python
# ai-engine/main.py
from fastapi import FastAPI
import mlflow
from langchain.chains import GraphCypherQAChain

app = FastAPI()

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("townin-graphrag")

# LangChain ìë™ ë¡œê¹…
mlflow.langchain.autolog()

@app.post("/graphrag/query")
async def graphrag_query(query: str):
    with mlflow.start_run(run_name=f"query_{query[:20]}"):
        mlflow.log_param("user_query", query)

        # GraphRAG ì‹¤í–‰ (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
        result = graphrag_pipeline.run(query)

        mlflow.log_metric("response_time_ms", result['latency'])
        mlflow.log_metric("tokens_used", result['tokens'])

        return result
```

---

## 5. MLflow + pm4py ì‹œë„ˆì§€

### 5.1 í†µí•© í™œìš© ì‹œë‚˜ë¦¬ì˜¤

**ê³¨ë“  ë£¨í‹´ ì‹¤í—˜ ì¶”ì **:
```python
import mlflow
import pm4py  # ìƒì—…ìš© ë¼ì´ì„ ìŠ¤ êµ¬ë§¤ í›„

mlflow.set_experiment("golden-routine-optimization")

# pm4py íŒŒë¼ë¯¸í„° ì‹¤í—˜
conformance_algorithms = ["token_replay", "alignments"]

for algorithm in conformance_algorithms:
    with mlflow.start_run(run_name=f"conformance_{algorithm}"):
        mlflow.log_param("algorithm", algorithm)
        mlflow.log_param("petri_net_variant", "inductive_miner")

        # pm4py conformance checking
        if algorithm == "token_replay":
            result = pm4py.token_replay(event_log, petri_net)
        else:
            result = pm4py.alignments(event_log, petri_net)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        mlflow.log_metrics({
            "fitness": result['fitness'],
            "precision": result['precision'],
            "processing_time_ms": result['time']
        })

        # ëª©í‘œ: fitness > 0.9, processing_time < 500ms
```

### 5.2 í†µí•© ëŒ€ì‹œë³´ë“œ

```python
# MLflow + pm4py í†µí•© ëŒ€ì‹œë³´ë“œ
import streamlit as st
import mlflow

st.title("Townin AI/Process Mining Dashboard")

# MLflow ì‹¤í—˜ ì¡°íšŒ
experiments = mlflow.search_runs(
    experiment_names=["townin-graphrag", "golden-routine"]
)

# GraphRAG ì„±ëŠ¥
st.subheader("GraphRAG Performance (PRD 9.1 ëª©í‘œì¹˜)")
st.metric("ê²€ìƒ‰ ì •ë°€ë„", f"{experiments['retrieval_precision'].mean():.1%}",
          delta="ëª©í‘œ: >85%")
st.metric("ì¶©ì‹¤ë„", f"{experiments['faithfulness'].mean():.1%}",
          delta="ëª©í‘œ: >95%")

# pm4py Golden Routine
st.subheader("Golden Routine Analysis")
st.metric("íŒ¨í„´ ì í•©ë„", f"{experiments['fitness'].mean():.2f}",
          delta="ëª©í‘œ: >0.9")
```

---

## 6. ì„±ê³µ ì§€í‘œ (PRD ì—…ë°ì´íŠ¸)

### 6.1 MLflow ìš´ì˜ ì§€í‘œ

| ì§€í‘œ | ì •ì˜ | ëª©í‘œì¹˜ |
|------|------|--------|
| **ì‹¤í—˜ ì¶”ì  ì»¤ë²„ë¦¬ì§€** | MLflowë¡œ ì¶”ì ë˜ëŠ” LLM í˜¸ì¶œ ë¹„ìœ¨ | > 95% |
| **ë¹„ìš© ê°€ì‹œì„±** | í† í° ë¹„ìš©ì´ ì¶”ì ë˜ëŠ” ë¹„ìœ¨ | > 90% |
| **ëª¨ë¸ ì¬í˜„ìœ¨** | MLflowë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ëª¨ë¸ ë¹„ìœ¨ | 100% |
| **í‰ê·  ì‹¤í—˜ ì£¼ê¸°** | ìƒˆ í”„ë¡¬í”„íŠ¸/íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ ì£¼ê¸° | < 1ì¼ |

### 6.2 PRD 9.1 RAG ì„±ëŠ¥ ì§€í‘œ (MLflow ìë™ ì¸¡ì •)

```python
# ìë™í™”ëœ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
with mlflow.start_run(run_name="daily_rag_benchmark"):
    metrics = mlflow.evaluate(
        model=graphrag_model,
        data=test_queries,
        metrics=["retrieval_precision", "faithfulness", "answer_relevancy"]
    )

    # PRD ëª©í‘œì¹˜ ë‹¬ì„± ì—¬ë¶€ ì•Œë¦¼
    if metrics['faithfulness'] < 0.95:
        send_alert("âš ï¸ Faithfulness dropped below 95% target!")
```

---

## 7. ë‹¤ìŒ ë‹¨ê³„

### 7.1 ì¦‰ì‹œ ì‹¤í–‰

- [ ] MLflow Docker í™˜ê²½ êµ¬ì¶•
- [ ] PostgreSQL + S3 ì„¤ì •
- [ ] LangChain ìë™ ë¡œê¹… í…ŒìŠ¤íŠ¸
- [ ] ì „ë‹¨ì§€ OCR ì‹¤í—˜ ì‹œì‘

### 7.2 1ì£¼ì¼ ì´ë‚´

- [ ] GraphRAG íŒŒì´í”„ë¼ì¸ì— MLflow í†µí•©
- [ ] í”„ë¡¬í”„íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ PoC
- [ ] í† í° ë¹„ìš© ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### 7.3 1ê°œì›” ì´ë‚´

- [ ] RAG ì„±ëŠ¥ ì§€í‘œ ìë™ ì¸¡ì • ì‹œìŠ¤í…œ
- [ ] FP ì½”íŒŒì¼ëŸ¿ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- [ ] ì‚¬ê¸° íƒì§€ ëª¨ë¸ ì‹¤í—˜ ì‹œì‘

### 7.4 Phase 2 ì°©ìˆ˜ ì‹œ

- [ ] pm4py + MLflow í†µí•© (ìƒì—…ìš© ë¼ì´ì„ ìŠ¤ êµ¬ë§¤ ì‹œ)
- [ ] ê³¨ë“  ë£¨í‹´ ì‹¤í—˜ ì¶”ì 
- [ ] MLflow UIë¥¼ NestJS ëŒ€ì‹œë³´ë“œì— ì„ë² ë“œ

---

## 8. ëŒ€ì•ˆ ë„êµ¬ ë¹„êµ

| ë„êµ¬ | ì¥ì  | ë‹¨ì  | ê²°ë¡  |
|------|------|------|------|
| **MLflow** | LangChain ë„¤ì´í‹°ë¸Œ, ì˜¤í”ˆì†ŒìŠ¤ | UI ë‹¤ì†Œ íˆ¬ë°• | âœ… **ìµœì ** |
| **Weights & Biases** | ì•„ë¦„ë‹¤ìš´ UI, í˜‘ì—… ê¸°ëŠ¥ | LangChain í†µí•© ì•½í•¨ | âŒ ë¶€ì í•© |
| **LangSmith** | LangChain ì „ìš©, íŠ¸ë ˆì´ì‹± ê°•ë ¥ | ìœ ë£Œ, ë²¤ë” ë½ì¸ | â–³ ë³´ì¡° ë„êµ¬ |
| **Neptune.ai** | ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ìš°ìˆ˜ | LLM ê¸°ëŠ¥ ì•½í•¨ | âŒ ë¶€ì í•© |

**ê¶Œì¥ ì¡°í•©**:
- **Primary**: MLflow (ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬)
- **Secondary**: LangSmith (ë””ë²„ê¹…ìš©, Phase 3 ì´í›„)

---

## 9. PRD ì—…ë°ì´íŠ¸ ì œì•ˆ

### 9.1 ê¸°ìˆ  ìŠ¤íƒ ì¶”ê°€

**8. ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)**

```yaml
Backend:
  AI Engine: Python (FastAPI)
    - LangChain (GraphRAG)
    - pm4py (ìƒì—…ìš© ë˜ëŠ” ìì²´ êµ¬í˜„)
    - MLflow (í•„ìˆ˜ ì¸í”„ë¼)  # ì‹ ê·œ

Infrastructure:
  - Docker, Kubernetes
  - AWS or Google Cloud
  - MLflow Tracking Server  # ì‹ ê·œ
  - PostgreSQL (MLflow ë©”íƒ€ë°ì´í„°)  # ì‹ ê·œ
  - S3 (MLflow ì•„í‹°íŒ©íŠ¸)  # ì‹ ê·œ
```

### 9.2 ì„±ê³µ ì§€í‘œ ìë™í™”

**9.1. RAG ì„±ëŠ¥ ì§€í‘œ (MLflow ìë™ ì¸¡ì •)**

```python
# ë§¤ì¼ ìë™ ì‹¤í–‰
@scheduler.scheduled_job('cron', hour=2)
def daily_rag_benchmark():
    with mlflow.start_run():
        metrics = mlflow.evaluate(
            model=graphrag_model,
            data=test_queries,
            metrics=[
                "retrieval_precision",  # ëª©í‘œ: > 85%
                "faithfulness",          # ëª©í‘œ: > 95%
                "answer_relevancy"       # ëª©í‘œ: > 90%
            ]
        )

        # Slack ì•Œë¦¼
        if metrics['faithfulness'] < 0.95:
            send_slack_alert("GraphRAG faithfulness ëª©í‘œ ë¯¸ë‹¬!")
```

---

## 10. ê²°ë¡ 

### 10.1 í•µì‹¬ ë©”ì‹œì§€

**MLflowëŠ” Townin í”„ë¡œì íŠ¸ì˜ í•„ìˆ˜ ì¸í”„ë¼ì…ë‹ˆë‹¤.**

**ì´ìœ **:
1. âœ… GraphRAG ìµœì í™” ì—†ì´ëŠ” ì„±ê³µ ì§€í‘œ(ê²€ìƒ‰ ì •ë°€ë„ 85%, ì¶©ì‹¤ë„ 95%) ë‹¬ì„± ë¶ˆê°€
2. âœ… LLM ë¹„ìš© ì¶”ì  ì—†ì´ëŠ” "GraphRAG êµ¬ì¶• ë¹„ìš© ê³¼ë‹¤" ë¦¬ìŠ¤í¬ ì™„í™” ë¶ˆê°€
3. âœ… í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ ì—†ì´ëŠ” í™˜ê° ë°©ì§€(FR-INS-003) ë³´ì¥ ë¶ˆê°€
4. âœ… 1ì¸ ìš´ì˜ ëª¨ë¸ì—ì„œ ìˆ˜ë™ ì‹¤í—˜ ì¶”ì ì€ ë¹„í˜„ì‹¤ì 

### 10.2 pm4py vs MLflow

| í•­ëª© | pm4py | MLflow |
|------|-------|--------|
| **íƒ€ë‹¹ì„± ì ìˆ˜** | 85/100 | **95/100** |
| **í•„ìˆ˜ì„±** | ì„ íƒì  (ì°¨ë³„í™”) | **í•„ìˆ˜** (ì¸í”„ë¼) |
| **ë„ì… ì‹œê¸°** | Phase 2ë¶€í„° | **Phase 1ë¶€í„°** |
| **ë¹„ìš©** | $5K-50K/ë…„ ë˜ëŠ” ìì²´ êµ¬í˜„ | **$60/ì›” (Self-hosted)** |

### 10.3 ìµœì¢… ê¶Œì¥ì‚¬í•­

```
Phase 1 (ì¦‰ì‹œ):
âœ… MLflow ì¸í”„ë¼ êµ¬ì¶• (Docker)
âœ… LangChain + MLflow í†µí•© í…ŒìŠ¤íŠ¸
âœ… ì „ë‹¨ì§€ OCR ì‹¤í—˜ ì‹œì‘

Phase 2:
âœ… GraphRAG ìµœì í™” í•µì‹¬ ë„êµ¬ë¡œ í™œìš©
âœ… pm4py + MLflow í†µí•© (pm4py ìƒì—…ìš© êµ¬ë§¤ ì‹œ)
âœ… RAG ì„±ëŠ¥ ì§€í‘œ ëŒ€ì‹œë³´ë“œ

Phase 3+:
âœ… ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¡œ ì‚¬ê¸° íƒì§€ ìš´ì˜
âœ… FP ì½”íŒŒì¼ëŸ¿ A/B í…ŒìŠ¤íŠ¸
âœ… Databricks Managed MLflow ë§ˆì´ê·¸ë ˆì´ì…˜ ê³ ë ¤
```

---

**ë¬¸ì„œ ìƒíƒœ**: âœ… ìµœì¢… ê²€í†  ì™„ë£Œ
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-12-21
**ë‹´ë‹¹ì**: Townin Development Team
**ìš°ì„ ìˆœìœ„**: ğŸ”´ **Critical** (Phase 1ë¶€í„° í•„ìˆ˜)

**MLflow ê³µì‹ ì‚¬ì´íŠ¸**: https://mlflow.org
**MLflow LLM Tracking**: https://mlflow.org/docs/latest/llms/index.html
**LangChain Integration**: https://python.langchain.com/docs/integrations/providers/mlflow_tracking/
